# GENERATIVE-TEXT-MODEL

Company : CODTECH IT SOLUTIONS

Name : Pravin S

Intern Id : CT04DG2370

Domain : ARTIFICIAL INTELLIGENCE

Duration : 4 Weeks

Mentor : Neela Santosh

Description:

This Python script demonstrates a comparison of text generation using two models: GPT-2 and an LSTM neural network. It begins by setting up the GPT-2 transformer model from Hugging Face's Transformers library. The model is loaded along with its tokenizer, and then used to generate text based on a given prompt ("The future of space exploration is"). GPT-2 generates a coherent continuation of the prompt using probabilistic sampling.

The second part of the script focuses on building and training a simple LSTM (Long Short-Term Memory) model using Keras. A small corpus of sentences related to artificial intelligence is tokenized and transformed into a series of input sequences. These sequences are used to train the LSTM model to predict the next word in a sentence. After training, the model can generate new text given a seed phrase (in this case, "Artificial intelligence").

Finally, the script uses matplotlib to create an image combining the outputs of both GPT-2 and the LSTM model. It wraps the text nicely and saves the result in an image file (text_generation_output.png), offering a visual comparison of the styles and coherence of both models' outputs.

Output:

![Image](https://github.com/user-attachments/assets/1f3c4d64-8b08-431f-bc4d-8300349f6822)
